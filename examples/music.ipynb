{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Music with xLSTM.\n",
    "\n",
    "This notebook shows you how to compose music with xLSTM\n",
    "\n",
    "On top of the installation instructions from the README, you might want to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomments. This is for the first time setup\n",
    "#!pip install note-seq\n",
    "#!pip install --upgrade bokeh==2.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the language model.\n",
    "\n",
    "We will use this model: https://huggingface.co/TristanBehrens/jsfakes-music-xlstm/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from source.languagemodel import LanguageModel\n",
    "import note_seq\n",
    "\n",
    "# Select a model to load.\n",
    "#model_path_or_repo = \"TristanBehrens/jsfakes-music-transformer\"\n",
    "#model_path_or_repo = \"TristanBehrens/jsfakes-music-pharia\"\n",
    "#model_path_or_repo = \"TristanBehrens/jsfakes-music-mamba\"\n",
    "#model_path_or_repo = \"TristanBehrens/jsfakes-music-xlstm\"\n",
    "#model_path_or_repo = \"TristanBehrens/bach-garland-transformer\"\n",
    "model_path_or_repo = \"TristanBehrens/bach-garland-pharia\"\n",
    "#model_path_or_repo = \"TristanBehrens/bach-garland-mamba\"\n",
    "#model_path_or_repo = \"TristanBehrens/bach-garland-xlstm\"\n",
    "#model_path_or_repo = \"TristanBehrens/bach-garland-mambaplus\"\n",
    "\n",
    "\n",
    "# Load the model.\n",
    "model = LanguageModel(\n",
    "    model_path_or_repo,\n",
    "    config_overrides={\"context_length\": 16_384},\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility code for music\n",
    "\n",
    "Here is some code that will map anything that the model generates into a music representation that is close to GM MIDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def handle_garland_tokens(tokens):\n",
    "    song_data = {}\n",
    "\n",
    "    song_data[\"tracks\"] = []\n",
    "\n",
    "    current_track_index = 0\n",
    "    current_timestep = 0\n",
    "    for token in tokens:\n",
    "        if token == \"GARLAND_START\":\n",
    "            pass\n",
    "        elif token == \"BAR_START\":\n",
    "            if current_track_index == len(song_data[\"tracks\"]):\n",
    "                song_data[\"tracks\"] += [{\"bars\": [], \"instrument\": \"0\"}]\n",
    "            bar_data = {\"notes\": []}\n",
    "            song_data[\"tracks\"][current_track_index][\"bars\"] += [bar_data]\n",
    "            current_timestep = 0\n",
    "        elif token.startswith(\"INST=\"):\n",
    "            instrument = token.split(\"=\")[1]\n",
    "            song_data[\"tracks\"][current_track_index][\"instrument\"] = instrument\n",
    "        elif token.startswith(\"DENSITY=\"):\n",
    "            pass\n",
    "        elif token.startswith(\"NOTE_ON=\"):\n",
    "            note_pitch = int(token.split(\"=\")[1])\n",
    "            note_data = {\n",
    "                \"note\": note_pitch,\n",
    "                \"start\": current_timestep,\n",
    "                \"end\": current_timestep,\n",
    "                \"veloctiy\": 80\n",
    "            }\n",
    "            song_data[\"tracks\"][current_track_index][\"bars\"][-1][\"notes\"] += [note_data]\n",
    "            pass\n",
    "        elif token.startswith(\"TIME_DELTA=\"):\n",
    "            current_timestep += int(token.split(\"=\")[1])\n",
    "        elif token.startswith(\"NOTE_OFF=\"):\n",
    "            note_pitch = int(token.split(\"=\")[1])\n",
    "            for note_data in song_data[\"tracks\"][current_track_index][\"bars\"][-1][\"notes\"]:\n",
    "                if note_data[\"note\"] == note_pitch and note_data[\"start\"] == note_data[\"end\"]:\n",
    "                    note_data[\"end\"] = current_timestep\n",
    "                    break\n",
    "            pass\n",
    "        elif token == \"BAR_END\":\n",
    "            current_track_index += 1\n",
    "        elif token == \"NEXT\":\n",
    "            current_track_index = 0\n",
    "        elif token == \"GARLAND_END\":\n",
    "            pass\n",
    "        elif token == \"[PAD]\":\n",
    "            pass\n",
    "        elif token == \"[EOS]\":\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(f\"Unknown token: {token}\")\n",
    "    \n",
    "    assert isinstance(song_data, dict)\n",
    "    return song_data\n",
    "\n",
    "def convert_songdata_to_notesequence(song_data:dict, quantize_steps_per_quarter=8, remove_disabled_tracks=True):\n",
    "\n",
    "    assert isinstance(song_data, dict), f\"Invalid song data type: {type(song_data)}\"\n",
    "\n",
    "    # Clone the song data.\n",
    "    song_data = copy.deepcopy(song_data)\n",
    "\n",
    "    # Sort the tracks by instrument.\n",
    "    assert \"tracks\" in song_data, f\"Invalid song data: {song_data.keys()}\"\n",
    "    tracks = sorted(song_data[\"tracks\"], key=lambda t: t[\"instrument\"])\n",
    "    song_data[\"tracks\"] = tracks\n",
    "\n",
    "    # Remove tracks that are not enabled.\n",
    "    if remove_disabled_tracks:\n",
    "        song_data[\"tracks\"] = [t for t in song_data[\"tracks\"] if t.get(\"enabled\", True)]\n",
    "\n",
    "    # Create an empy note sequence.\n",
    "    note_sequence = note_seq.protobuf.music_pb2.NoteSequence()\n",
    "\n",
    "    # Add the tempo.\n",
    "    bpm = song_data[\"bpm\"] if \"bpm\" in song_data else 120\n",
    "    note_sequence.tempos.add().qpm = bpm\n",
    "\n",
    "    # Compute some lengths.\n",
    "    step_length_seconds = 60.0 / bpm / quantize_steps_per_quarter\n",
    "    bar_length_seconds = 4 * step_length_seconds * quantize_steps_per_quarter\n",
    "\n",
    "    # Get the instruments.\n",
    "    instruments = list(set([t[\"instrument\"] for t in song_data[\"tracks\"]]))\n",
    "\n",
    "    # Add the tracks.\n",
    "    for track_index, track_data in enumerate(song_data[\"tracks\"]):\n",
    "        instrument = track_data[\"instrument\"]\n",
    "        for bar_index, bar_data in enumerate(track_data[\"bars\"]):\n",
    "            bar_start_time = bar_index * bar_length_seconds\n",
    "            for note_data in bar_data[\"notes\"]:\n",
    "                assert \"note\" in note_data\n",
    "                assert \"start\" in note_data\n",
    "                assert \"end\" in note_data\n",
    "                note = note_sequence.notes.add()\n",
    "                #note.instrument = instrument TODO\n",
    "                note.pitch = note_data[\"note\"]\n",
    "                note.start_time = note_data[\"start\"] * step_length_seconds + bar_start_time\n",
    "                note.end_time = note_data[\"end\"] * step_length_seconds + bar_start_time\n",
    "                if \"velocity\" in note_data:\n",
    "                    note.velocity = note_data[\"velocity\"]\n",
    "                else:\n",
    "                    note.velocity = 80\n",
    "                note.instrument = track_index\n",
    "                if instrument == \"drums\":\n",
    "                    note.is_drum = True\n",
    "                else:\n",
    "                    note.is_drum = False\n",
    "                    note.program = int(instrument)\n",
    "\n",
    "    return note_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The temperature of the generation. The higher the temperature, the more random the output.\n",
    "temperature = 0.5\n",
    "\n",
    "# The maximum length of the generated music.\n",
    "max_length = 16_384\n",
    "\n",
    "# When to stop the generation.\n",
    "end_tokens = [\"NEXT\"]\n",
    "\n",
    "# Compose the music iterativelybar by bar.\n",
    "output = \"GARLAND_START\"\n",
    "for iteration in range(10):\n",
    "    output_dict = model.generate(\n",
    "        prompt=output,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        end_tokens=end_tokens,\n",
    "        forbidden_tokens=[\"[PAD]\", \"[EOS]\", \"GARLAND_END\"],\n",
    "        return_structured_output=True\n",
    "    )\n",
    "    for key, value in output_dict.items():\n",
    "        print(key, value)\n",
    "    output = output_dict[\"output\"]\n",
    "    length = len(output.split(\" \"))\n",
    "    print(f\"Iteration {iteration}, length {length:_}\")\n",
    "    if length >= max_length:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = output.split()\n",
    "song_data = handle_garland_tokens(tokens)\n",
    "note_sequence = convert_songdata_to_notesequence(song_data)\n",
    "\n",
    "note_seq.plot_sequence(note_sequence)\n",
    "note_seq.play_sequence(note_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
